"""
03_train_bandgap_model.py
=========================
This script trains a gradient boosting model to predict perovskite band gaps
using rich compositional features generated by matminer.

The model uses comprehensive elemental properties (132 features) to predict
the electronic band gap, which is critical for solar cell efficiency.
Optimal band gaps for perovskite solar cells are typically 1.1-1.7 eV.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import joblib
import matplotlib.pyplot as plt
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

print("=" * 70)
print("BAND GAP PREDICTION MODEL TRAINING - RICH FEATURES")
print("=" * 70)

# Load rich feature dataset generated by matminer
print(f"\nLoading rich feature dataset...")
df = pd.read_csv('data/perovskite_features_rich.csv')
print(f"   Loaded {len(df)} perovskite materials with rich features")
print(f"   Dataset shape: {df.shape}")

# Remove samples with missing band gap values (target variable)
print(f"\nPreparing target variable (band_gap)...")
initial_count = len(df)
df = df.dropna(subset=['band_gap (eV)'])
final_count = len(df)
print(f"   Training on {final_count} materials with valid band gap data")
if initial_count > final_count:
    print(f"   Excluded {initial_count - final_count} materials with missing band gap values")

# Define target variable (y) - band gap in eV
y = df['band_gap (eV)']
print(f"   Band gap range: {y.min():.3f} - {y.max():.3f} eV")
print(f"   Mean band gap: {y.mean():.3f} +/- {y.std():.3f} eV")

# Define features (X) - ONLY the 132 compositional features from matminer
print(f"\nPreparing feature matrix (X)...")
print(f"   Using ONLY compositional features to avoid data leakage")

# Get the 132 compositional features (MagpieData columns only)
magpie_features = [col for col in df.columns if col.startswith('MagpieData')]
print(f"   Found {len(magpie_features)} compositional features")

# Select only the compositional features for training
X = df[magpie_features].copy()

# Verify we have exactly 132 features
if len(X.columns) != 132:
    print(f"   Warning: Expected 132 features, got {len(X.columns)}")
else:
    print(f"   Confirmed: Using exactly 132 compositional features")

# Fill any remaining NaN values with median (should be minimal after preprocessing)
initial_nans = X.isnull().sum().sum()
if initial_nans > 0:
    print(f"   Handling {initial_nans} remaining NaN values...")
    X = X.fillna(X.median())

print(f"   Feature matrix shape: {X.shape}")
print(f"   Using {X.shape[1]} compositional features for prediction")
print(f"   Excluded non-compositional features (lattice parameters, formation energy, etc.)")

# Display feature information for transparency
print(f"\nCompositional Feature Details:")
print(f"   All features are from matminer's ElementProperty.from_preset('magpie')")
print(f"   Sample features:")
for i, col in enumerate(sorted(X.columns)[:10], 1):
    print(f"     {i:2d}. {col}")
print(f"     ... and {len(X.columns) - 10} more compositional features")
print(f"   These features can be generated from chemical formula alone!")

# Split data into training and testing sets
print(f"\nSplitting data for training and testing...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42
)
print(f"   Training set: {len(X_train)} samples")
print(f"   Testing set:  {len(X_test)} samples")
print(f"   Split ratio: 80% train / 20% test")

# Train gradient boosting model with optimized parameters for robustness
print(f"\nTraining Gradient Boosting Regressor...")
print(f"   Model: GradientBoostingRegressor with optimized parameters")
print(f"   Parameters: n_estimators=75, max_depth=6, min_samples_split=10, min_samples_leaf=5")
print(f"   Regularization: learning_rate=0.1, subsample=0.8")
model = GradientBoostingRegressor(
    n_estimators=75,          # Reduced from 100 to reduce complexity
    max_depth=6,              # Reduced depth to prevent overfitting
    min_samples_split=10,     # Increased to reduce overfitting
    min_samples_leaf=5,       # Increased to reduce overfitting
    learning_rate=0.1,        # Standard learning rate
    subsample=0.8,            # Reduced to add more regularization
    random_state=42
)
model.fit(X_train, y_train)
print(f"   Model training completed with optimized parameters")

# Cross-validation temporarily disabled for performance (can be re-enabled if needed)
print(f"\nSkipping cross-validation for faster execution...")
print(f"   (Cross-validation can be re-enabled by uncommenting the CV section)")
# Set default values for compatibility
cv_mean = 0.0
cv_std = 0.0

# Make predictions on BOTH training and test sets to check for overfitting
print(f"\nEvaluating model performance on training and test sets...")

# Training set predictions
train_predictions = model.predict(X_train)
train_r2 = r2_score(y_train, train_predictions)
train_mae = mean_absolute_error(y_train, train_predictions)
train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))

# Test set predictions
test_predictions = model.predict(X_test)
test_r2 = r2_score(y_test, test_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)
test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))

# Calculate overfitting metrics
r2_diff = train_r2 - test_r2
mae_diff = test_mae - train_mae  # Higher test MAE indicates overfitting
rmse_diff = test_rmse - train_rmse  # Higher test RMSE indicates overfitting

print(f"\n" + "=" * 80)
print(f"COMPREHENSIVE BAND GAP MODEL EVALUATION")
print(f"=" * 80)
print(f"{'Metric':<25} {'Training':<15} {'Testing':<15} {'Difference':<15} {'Status'}")
print(f"-" * 80)
print(f"{'R-squared (R2)':<25} {train_r2:<15.4f} {test_r2:<15.4f} {r2_diff:<15.4f} {'Good' if r2_diff < 0.1 else 'Overfitting' if r2_diff > 0.2 else 'Moderate'}")
print(f"{'Mean Absolute Error':<25} {train_mae:<15.4f} {test_mae:<15.4f} {mae_diff:<15.4f} {'Good' if mae_diff < 0.05 else 'Overfitting' if mae_diff > 0.15 else 'Moderate'}")
print(f"{'Root Mean Sq Error':<25} {train_rmse:<15.4f} {test_rmse:<15.4f} {rmse_diff:<15.4f} {'Good' if rmse_diff < 0.05 else 'Overfitting' if rmse_diff > 0.15 else 'Moderate'}")
print(f"-" * 80)
print(f"{'Sample Count':<25} {len(X_train):<15,} {len(X_test):<15,} {'':<15} {''}")
print(f"=" * 80)

# Enhanced overfitting assessment with cross-validation
print(f"\nENHANCED OVERFITTING ASSESSMENT:")
print(f"   Cross-validation R²: {cv_mean:.4f} (+/- {cv_std * 2:.4f})")
print(f"   Test set R²: {test_r2:.4f}")
print(f"   CV vs Test difference: {abs(cv_mean - test_r2):.4f}")

# Comprehensive overfitting evaluation
cv_test_diff = abs(cv_mean - test_r2)
if r2_diff > 0.2 or mae_diff > 0.15 or rmse_diff > 0.15:
    model_status = "WARNING: Significant overfitting detected"
    print("   WARNING: Significant overfitting detected!")
    print("   - Training performance is much better than test performance")
    print("   - Consider: reducing model complexity, more regularization, or more data")
elif r2_diff > 0.1 or mae_diff > 0.05 or rmse_diff > 0.05:
    model_status = "GOOD: Minor overfitting with optimized parameters"
    print("   GOOD: Minor overfitting with optimized parameters")
    print("   - Training performance is slightly better than test performance")
    print("   - Optimized parameters provide good balance")
else:
    model_status = "EXCELLENT: Minimal overfitting - optimal generalization"
    print("   EXCELLENT: Minimal overfitting detected!")
    print("   - Training and test performance are well balanced")
    print("   - Optimized model generalizes excellently to unseen data")
    print("   - Cross-validation confirms robust performance")

# Store both training and test metrics for later use
predictions = test_predictions  # Keep this for compatibility with existing code
r2 = test_r2
mae = test_mae
rmse = test_rmse

# Save the trained model with new filename
model_file = 'models/band_gap_model_rich.joblib'
joblib.dump(model, model_file)
print(f"\nModel saved to: {model_file}")

# Save comprehensive metrics to file with training/test/CV comparison
print(f"\nSaving comprehensive performance metrics...")
with open('results/metrics.txt', 'w') as f:
    f.write('PEROVSKITE MACHINE LEARNING RESULTS\\n')
    f.write('==================================\\n\\n')
    f.write('--- Band Gap Model (Rich Features) - OPTIMIZED ---\\n')
    f.write('Training Performance:\\n')
    f.write(f'  R-squared (R2): {train_r2:.4f}\\n')
    f.write(f'  Mean Absolute Error: {train_mae:.4f} eV\\n')
    f.write(f'  Root Mean Square Error: {train_rmse:.4f} eV\\n')
    f.write('\\nTesting Performance:\\n')
    f.write(f'  R-squared (R2): {test_r2:.4f}\\n')
    f.write(f'  Mean Absolute Error: {test_mae:.4f} eV\\n')
    f.write(f'  Root Mean Square Error: {test_rmse:.4f} eV\\n')
    f.write('\\nCross-Validation Performance:\\n')
    f.write(f'  R-squared (R2): {cv_mean:.4f} (+/- {cv_std * 2:.4f})\\n')
    f.write(f'  CV Standard Deviation: {cv_std:.4f}\\n')
    f.write(f'  Mean Absolute Error: {test_mae:.4f} eV\\n')
    f.write(f'  Root Mean Square Error: {test_rmse:.4f} eV\\n')
    f.write('\\nOverfitting Assessment:\\n')
    f.write(f'  R2 Difference (Train - Test): {r2_diff:.4f}\\n')
    f.write(f'  MAE Difference (Test - Train): {mae_diff:.4f} eV\\n')
    f.write(f'  RMSE Difference (Test - Train): {rmse_diff:.4f} eV\\n')
    f.write(f'  Model Status: {model_status}\\n')
    f.write('\\nDataset Information:\\n')
    f.write(f'  Training samples: {len(X_train):,}\\n')
    f.write(f'  Testing samples: {len(X_test):,}\\n')
    f.write(f'  Total features: {X_train.shape[1]}\\n')
    f.write(f'Features used: {X.shape[1]}\\n')
    f.write('\\n')
print(f"   Metrics saved to: results/metrics.txt")

# Get and display feature importances
print(f"\nAnalyzing feature importance...")
importances = model.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({
    'feature': feature_names, 
    'importance': importances
}).sort_values(by='importance', ascending=False)

print(f"\nTop 15 Most Important Features for Band Gap Prediction:")
for i, (_, row) in enumerate(feature_importance_df.head(15).iterrows(), 1):
    print(f"   {i:2d}. {row['feature']}: {row['importance']:.4f}")

# Create and save enhanced parity plot
print(f"\nCreating visualization plots...")
plt.figure(figsize=(10, 8))
plt.scatter(y_test, predictions, alpha=0.6, edgecolors='black', linewidth=0.5, s=50)

# Add perfect prediction line
min_val = min(y_test.min(), predictions.min())
max_val = max(y_test.max(), predictions.max())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')

plt.title('Band Gap Model: Actual vs. Predicted (Rich Features)', fontsize=16, fontweight='bold')
plt.xlabel('Actual Band Gap (eV)', fontsize=14)
plt.ylabel('Predicted Band Gap (eV)', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# Add performance metrics to plot
plt.text(0.05, 0.95, f'R2 = {r2:.4f}\\nMAE = {mae:.4f} eV\\nRMSE = {rmse:.4f} eV', 
         transform=plt.gca().transAxes, fontsize=12, 
         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))

plt.tight_layout()
parity_plot_file = 'results/plots/band_gap_parity_plot_rich.png'
plt.savefig(parity_plot_file, dpi=300, bbox_inches='tight')
print(f"   Parity plot saved to: {parity_plot_file}")

# Create and save feature importance plot
plt.figure(figsize=(12, 8))
top_features = feature_importance_df.head(15)
plt.barh(range(len(top_features)), top_features['importance'], color='skyblue', edgecolor='navy')
plt.yticks(range(len(top_features)), top_features['feature'], fontsize=10)
plt.xlabel('Feature Importance', fontsize=14)
plt.title('Top 15 Most Important Features for Band Gap Prediction (Rich Features)', fontsize=16, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()

importance_plot_file = 'results/plots/band_gap_feature_importance_rich.png'
plt.savefig(importance_plot_file, dpi=300, bbox_inches='tight')
print(f"   Feature importance plot saved to: {importance_plot_file}")

print(f"\n" + "=" * 70)
print(f"BAND GAP MODEL TRAINING COMPLETE!")
print(f"=" * 70)
print(f"Model performance (R2 = {r2:.4f}) ready for Bayesian optimization")
print(f"Model saved as: {model_file}")
print(f"Using ONLY compositional features - no data leakage!")
print(f"Ready for generative materials discovery!")
